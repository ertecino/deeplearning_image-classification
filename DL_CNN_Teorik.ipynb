{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Derin Öğrenme Model Mimarisi: Evrişimsel Sinir Ağları (CNN) Uygulaması\n",
        "\n",
        "Bu ders notu, modern bir Evrişimsel Sinir Ağı (CNN) modelinin temel bileşenlerini ve çalışma prensiplerini açıklamaktadır. Sağlanan Python kodu üzerinden katmanların işlevlerini ve mimarinin genel yapısını inceleyeceğiz.\n",
        "\n",
        "## Model Mimarisine Genel Bakış\n",
        "\n",
        "Bu model, Keras Functional API kullanılarak oluşturulmuş bir **Xception** benzeri mimaridir. Özellikle görüntü sınıflandırma görevleri için tasarlanmıştır. Modelin temel amacı, girdi görüntülerinden anlamlı özellikler çıkararak doğru sınıflandırma tahmini yapmaktır.\n",
        "\n",
        "---\n",
        "\n",
        "## Model Katmanları ve Fonksiyonları\n",
        "\n",
        "Aşağıda, model kodunda kullanılan her bir katman ve işlevinin detaylı açıklaması bulunmaktadır:\n",
        "\n",
        "### 1. Giriş Katmanı (Input Layer)"
      ],
      "metadata": {
        "id": "1caTH1NGeFaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=input_shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ZhYS5duueFaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Açıklama:** Modelin alacağı verinin şeklini tanımlar. `input_shape`, genellikle `(yükseklik, genişlik, kanal_sayısı)` formatında olur (örneğin, `(180, 180, 3)` renkli bir resim için). Bu katman, modelin beklediği girdi formatını belirler.\n",
        "\n",
        "### 2. Yeniden Ölçeklendirme (Rescaling)"
      ],
      "metadata": {
        "id": "HrQ2VJr6eFaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.Rescaling(1.0 / 255)(inputs)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "AohvULWWeFaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Açıklama:** Girdi görüntülerini piksel değerleri genellikle 0-255 arasında değişir. Bu katman, değerleri **[0, 1]** aralığına normalize eder.\n",
        "* **Neden Kullanılır?** Bu **standardizasyon**, sinir ağlarının daha kararlı ve hızlı öğrenmesini sağlar, çünkü çok büyük veya çok küçük değerler gradyan patlaması/sönümlenmesi gibi sorunlara yol açabilir.\n",
        "\n",
        "### 3. Evrişimsel Katman (Conv2D)"
      ],
      "metadata": {
        "id": "NeiZgDLSeFaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "jVkbR2hNeFaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Açıklama:** Görüntüdeki yerel desenleri ve **özellikleri (kenarlar, köşeler, dokular)** çıkarmak için kullanılır. `128` farklı **filtre (kernel)** ile girdi üzerinde tarama yapar. `3` filtre boyutu (3x3).\n",
        "* **`strides=2`:** Her adımda 2 piksel atlayarak ilerler, bu da çıktı boyutunu **yarıya indirir**.\n",
        "* **`padding=\"same\"`:** Çıktı özelliğinin boyutunun, girdi özelliğinin boyutuna (stride dikkate alınarak) \"aynı\" kalmasını sağlamak için kenarlara dolgu (padding) ekler.\n",
        "\n",
        "### 4. Toplu Normalizasyon (BatchNormalization)"
      ],
      "metadata": {
        "id": "TH-NusB5eFaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.BatchNormalization()(x)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "odvDZ3TLeFaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Açıklama:** Her bir eğitim **mini-batch'i** içindeki aktivasyonların ortalamasını ve varyansını normalize eder. Yani her bir katmanın çıktısını belirli bir dağılıma (genellikle ortalama 0, varyans 1) sahip olacak şekilde ayarlar.\n",
        "* **Neden Kullanılır?** Bu, **iç kovaryat kaymasını (internal covariate shift)** azaltarak modelin daha hızlı eğitilmesine, daha kararlı olmasına ve daha iyi genelleme yapmasına yardımcı olur. Aşırı uyumu (overfitting) bir miktar azaltmaya da yardımcı olabilir.\n",
        "\n",
        "### 5. Aktivasyon Fonksiyonu (Activation - ReLU)"
      ],
      "metadata": {
        "id": "WV7caP4qeFaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.Activation(\"relu\")(x)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "rBY3kCO0eFab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Açıklama:** **ReLU (Rectified Linear Unit)**, $f(x) = \\max(0, x)$ formülüyle çalışan doğrusal olmayan bir aktivasyon fonksiyonudur. Negatif girdileri sıfıra dönüştürür, pozitif girdileri olduğu gibi bırakır.\n",
        "* **Neden Kullanılır?** Sinir ağlarına **doğrusal olmayanlık** kazandırır. Bu sayede ağ, daha karmaşık ilişkileri ve desenleri öğrenebilir. ReLU, yaygın olarak kullanılır çünkü hesaplaması basittir ve gradyan sönümlenmesi sorununu azaltmaya yardımcı olur.\n",
        "\n",
        "### 6. Ayrılabilir Evrişim (SeparableConv2D)"
      ],
      "metadata": {
        "id": "UoHmsWSYeFab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "sX4_2JgdeFae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Açıklama:** Standart `Conv2D` katmanına göre daha verimli bir evrişim türüdür. **Derinlemesine (Depthwise)** ve **Noktasal (Pointwise)** evrişimlerin birleşimidir.\n",
        "    * **Derinlemesine Evrişim:** Her bir giriş kanalını bağımsız olarak işler.\n",
        "    * **Noktasal Evrişim (1x1 Conv):** Derinlemesine evrişimin çıktısını birleştirerek yeni özellik haritaları oluşturur.\n",
        "* **Neden Kullanılır?** Daha az hesaplama maliyeti ve parametre sayısı gerektirir, bu da daha derin modellerin daha hızlı eğitilmesini sağlar ve aşırı uyumu azaltmaya yardımcı olabilir.\n",
        "\n",
        "### 7. Maksimum Havuzlama (MaxPooling2D)"
      ],
      "metadata": {
        "id": "WuZU1we2eFaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "wckmrwSKeFag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Açıklama:** Özellik haritalarının boyutunu küçültür (**downsampling**). Belirtilen boyutta (`3x3`) bir pencere üzerinde en yüksek piksel değerini seçer. `strides=2` ile çıktıyı yarıya indirir.\n",
        "* **Neden Kullanılır?**\n",
        "    * **Hesaplama Yükünü Azaltma:** Parametre sayısını ve hesaplama maliyetini düşürür.\n",
        "    * **Konumdan Bağımsızlık:** Modelin nesnelerin görüntüdeki küçük yer değişikliklerine karşı daha az hassas olmasını sağlar, yani bir özelliğin tam konumundan ziyade varlığına odaklanır.\n",
        "    * **Özellik Özetleme:** En belirgin özellikleri koruyarak gereksiz bilgiyi atar.\n",
        "\n",
        "### 8. Kalan Bağlantılar (Residual Connections / Skip Connections)"
      ],
      "metadata": {
        "id": "2Qv81teeeFah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
        "x = layers.add([x, residual])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "egN_FiG-eFah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Açıklama:** Bu kısım, **Derin Artık Ağlar (ResNet)** mimarilerinden ilham alan önemli bir özelliktir. Bir bloğun çıktısının ( `x` ) bir önceki bloğun çıktısına ( `previous_block_activation` ) eklenmesidir. `residual` bağlantısı, boyut uyumsuzluğunu gidermek için `1x1 Conv2D` kullanır.\n",
        "* **Neden Kullanılır?**\n",
        "    * **Gradyan Akışını Kolaylaştırma:** Çok derin ağlarda gradyanların kaybolması (vanishing gradients) sorununu hafifleterek eğitimin daha kararlı olmasını sağlar.\n",
        "    * **Performansı Artırma:** Modelin daha derin katmanlarda bile etkili bir şekilde öğrenmeye devam etmesini sağlar.\n",
        "    * **Kimlik Eşleme (Identity Mapping):** Blokların en azından bir kimlik eşleme öğrenmesini sağlar, yani ağırlıkları sıfır olsa bile bilgi akışı korunur.\n",
        "\n",
        "### 9. Global Ortalama Havuzlama (GlobalAveragePooling2D)"
      ],
      "metadata": {
        "id": "PlSqmcA8eFah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.GlobalAveragePooling2D()(x)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "3MgJxln_eFai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Açıklama:** Her bir özellik haritasının ortalamasını alır ve sonuç olarak her bir özellik haritası için tek bir skaler değer üretir. Uzamsal boyutları (genişlik ve yükseklik) tek bir değere sıkıştırır.\n",
        "* **Neden Kullanılır?**\n",
        "    * **Parametre Azaltma:** Tamamen bağlı katmanlardaki parametre sayısını önemli ölçüde azaltır.\n",
        "    * **Konumdan Bağımsızlık:** Modelin nesnenin uzamsal konumuna daha az bağlı olmasını sağlar.\n",
        "    * **Daha İyi Genelleme:** Genellikle aşırı uyumu azaltmaya yardımcı olur.\n",
        "\n",
        "### 10. Dropout Katmanı"
      ],
      "metadata": {
        "id": "VFe2jYMjeFai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.Dropout(0.25)(x)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "juQvfiOIeFaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Açıklama:** Eğitim sırasında, rastgele seçilen nöronların belirli bir yüzdesini (burada %25'ini) geçici olarak devre dışı bırakır. Bu, bu nöronların çıktıları sıfırlanır.\n",
        "* **Neden Kullanılır?**\n",
        "    * **Aşırı Uyum Engelleme:** Nöronların belirli özelliklere veya diğer nöronlara aşırı bağımlı olmasını önler. Her eğitim adımında farklı bir ağ yapılandırılmış gibi olur.\n",
        "    * **Model Sağlamlığı:** Modelin daha sağlam ve genellenebilir olmasını sağlar. **Yalnızca eğitim sırasında etkindir**, çıkarım (inference) aşamasında devre dışıdır.\n",
        "\n",
        "### 11. Yoğun Katman (Dense Layer)"
      ],
      "metadata": {
        "id": "XNV3JI9heFaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = layers.Dense(units, activation=None)(x)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FdP9vrHEeFaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Açıklama:** Tamamen bağlantılı (Fully Connected) katmandır. Girdideki tüm nöronlar, çıktıdaki tüm nöronlara bağlıdır.\n",
        "    * `units`: Sınıflandırma problemi türüne göre değişir:\n",
        "        * **İkili Sınıflandırma (`num_classes == 2`):** `units = 1`. Genellikle sigmoid aktivasyon ile birleştirilir (ancak burada `activation=None` olduğu için logit döndürülür).\n",
        "        * **Çoklu Sınıflandırma (`num_classes > 2`):** `units = num_classes`. Genellikle softmax aktivasyon ile birleştirilir (yine burada logit döndürülür).\n",
        "* **`activation=None`:** Bu, katmanın doğrudan **logitleri** (ham, normalize edilmemiş skorları) döndüreceği anlamına gelir. Kayıp fonksiyonu (örneğin `BinaryCrossentropy(from_logits=True)` veya `CategoricalCrossentropy(from_logits=True)`) bu logitleri kullanarak doğru bir şekilde kaybı hesaplar.\n",
        "\n",
        "---\n",
        "\n",
        "## Modelin Akışı (Xception Benzeri Mimari)\n",
        "\n",
        "Bu model, Google tarafından geliştirilen **Xception** mimarisinin temel prensiplerini yansıtır:\n",
        "\n",
        "1.  **Giriş Bloğu (Entry Flow):** İlk evrişim ve batch normalizasyon katmanları ile başlar. Resimleri ön işlemden geçirir ve temel özellikleri çıkarmaya başlar.\n",
        "2.  **Orta Akış Döngüsü (Middle Flow):** Modelin ana öğrenme bölümüdür. Art arda tekrarlanan ayrılabilir evrişim blokları ve aralarındaki kalan bağlantılar (residual connections) sayesinde derinlemesine özellik öğrenimi sağlanır. Her blok sonunda boyutu küçültmek için `MaxPooling2D` kullanılır.\n",
        "3.  **Çıkış Bloğu (Exit Flow):** Son ayrılabilir evrişim katmanları, ardından global ortalama havuzlama ve dropout ile modelin nihai özellikleri özetlenir ve aşırı uyum önlenir. En son yoğun katman, nihai sınıflandırma tahminlerini yapar.\n",
        "\n",
        "---\n",
        "\n",
        "Bu yapı, Dr. Murat Altun'un da vurguladığı gibi, derin öğrenme modellerinin verimli bir şekilde eğitilmesini ve karmaşık görüntü tanıma görevlerinde yüksek başarı elde etmesini sağlar. Özellikle **derin öğrenme modellerinin nasıl tasarlandığı, katmanların ne işe yaradığı ve neden kullanıldığı** gibi temel sorulara yanıt vermektedir."
      ],
      "metadata": {
        "id": "3A0TQUQUeFak"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}