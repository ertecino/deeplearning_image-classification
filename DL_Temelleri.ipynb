{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0qtVDQnfGT8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Derin Öğrenme (Deep Learning) Temelleri: Colab Ders Notu\n",
        "\n",
        "Bu ders notu, derin öğrenmenin temel kavramlarına ve popüler mimarilerine kapsamlı bir giriş sunmaktadır. Her bir başlık altında ilgili kavramları ve işlevlerini bulacaksınız.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Derin Öğrenmeye Giriş\n",
        "\n",
        "**Derin Öğrenme (Deep Learning)**, **Makine Öğrenimi'nin** bir alt dalıdır. Temel ilhamını insan beyninin çalışma prensibinden, özellikle de **biyolojik sinir ağlarından** alır.\n",
        "\n",
        "* **Amaç:** Çok katmanlı (yani \"derin\") yapay sinir ağları kullanarak verilerden **karmaşık özellikleri otomatik olarak öğrenmeyi** hedefler.\n",
        "* **Etkin Olduğu Alanlar:** Özellikle **görüntü, ses veya metin gibi yapılandırılmamış (unstructured) verilerde** son derece başarılıdır.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Temel Kavramlar\n",
        "\n",
        "### 2.1. Yapay Sinir Ağı (Artificial Neural Network - ANN)\n",
        "\n",
        "* **Tanım:** Birbirine bağlı **\"nöron\"** adı verilen işlem birimlerinden (düğümlerden) oluşur.\n",
        "* **İşleyiş:** Her nöron, girdi alır, üzerinde matematiksel işlemler yapar (çoğunlukla ağırlıklandırma ve toplama) ve bir çıktı üretir. Bu çıktı, genellikle bir sonraki katmandaki nöronlara girdi olarak verilir.\n",
        "\n",
        "### 2.2. Katmanlar (Layers)\n",
        "\n",
        "Sinir ağları, işlevlerine göre farklı katmanlar halinde düzenlenir:\n",
        "\n",
        "* **Girdi Katmanı (Input Layer):**\n",
        "    * Modelin ilk katmanıdır.\n",
        "    * Dış dünyadan **ham veriyi (görüntü pikselleri, ses sinyalleri, metin kelimeleri vb.)** doğrudan alır.\n",
        "* **Gizli Katmanlar (Hidden Layers):**\n",
        "    * Girdi ve çıktı katmanları arasında yer alan katmanlardır.\n",
        "    * Modelin en kritik öğrenme sürecinin gerçekleştiği yerdir.\n",
        "    * Girdiden **karmaşık ve soyut özellikleri** (örneğin, bir resimdeki kenarları, dokuları, daha sonra nesne parçalarını) öğrenirler.\n",
        "    * **Derin öğrenme**, genellikle **birden fazla gizli katmana** sahip ağları ifade eder; bu \"derinlik\" ağın daha karmaşık hiyerarşik temsilleri öğrenmesini sağlar.\n",
        "* **Çıktı Katmanı (Output Layer):**\n",
        "    * Ağın son katmanıdır.\n",
        "    * Modelin nihai **tahminini, sınıflandırmasını veya regresyon çıktısını** üretir.\n",
        "    * Nöron sayısı, çözülmeye çalışılan probleme göre değişir (örneğin, ikili sınıflandırma için 1 nöron, 10 sınıf için 10 nöron).\n",
        "\n",
        "### 2.3. Nöron (Neuron)\n",
        "\n",
        "* **Tanım:** Bir katmandaki temel **işlem birimidir**. Biyolojik bir nörona benzer şekilde, girdileri işler ve bir çıktı üretir.\n",
        "* **İşleyiş:**\n",
        "    1.  Girdileri alır.\n",
        "    2.  Her girdiyi kendi **ağırlığı (weight)** ile çarpar.\n",
        "    3.  Tüm ağırlıklı girdileri toplar ve bir **bias (önyargı)** değeri ekler.\n",
        "    4.  Elde edilen toplamı bir **aktivasyon fonksiyonundan** geçirerek nöronun nihai çıktısını üretir.\n",
        "\n",
        "### 2.4. Ağırlıklar (Weights) ve Biaslar (Biases)\n",
        "\n",
        "* **Ağırlıklar:**\n",
        "    * Modelin **öğrenme sürecinde ayarlanan temel parametrelerdir**.\n",
        "    * Bir girdinin (veya bir önceki nöronun çıktısının) bir sonraki nöronun çıktısını **ne kadar etkileyeceğini** belirler. Yüksek ağırlık, o girdinin daha önemli olduğu anlamına gelir.\n",
        "* **Biaslar:**\n",
        "    * Bir nöronun aktivasyon eşiğini (threshold) ayarlayan ek bir parametredir.\n",
        "    * Modelin belirli bir girdiye bakılmaksızın ne kadar aktif olması gerektiğini belirler.\n",
        "\n",
        "### 2.5. Aktivasyon Fonksiyonu (Activation Function)\n",
        "\n",
        "* **Tanım:** Nöronun ağırlıklı girdiler toplamına ve bias'a uyguladığı **doğrusal olmayan (non-linear)** bir fonksiyondur.\n",
        "* **Neden Önemli?**\n",
        "    * Sinir ağına **doğrusal olmayanlık** kazandırır. Eğer aktivasyon fonksiyonları olmasaydı veya sadece doğrusal olsalardı, ne kadar katman eklersek ekleyelim, ağ sadece doğrusal ilişkileri öğrenebilirdi ve karmaşık problemleri çözemezdi.\n",
        "    * Örnekler:\n",
        "        * **ReLU (Rectified Linear Unit):** $f(x) = \\max(0, x)$. Genellikle gizli katmanlarda tercih edilir, çünkü hesaplaması basittir ve gradyan sönümlenmesi sorununu azaltmaya yardımcı olur.\n",
        "        * **Sigmoid:** $f(x) = 1 / (1 + e^{-x})$. Çıktıyı 0 ile 1 arasına sıkıştırır. İkili sınıflandırma problemlerinin çıktı katmanında (olasılık tahmini için) kullanılabilir.\n",
        "        * **Tanh (Hyperbolic Tangent):** $f(x) = (e^x - e^{-x}) / (e^x + e^{-x})$. Çıktıyı -1 ile 1 arasına sıkıştırır.\n",
        "\n",
        "### 2.6. Geri Yayılım (Backpropagation)\n",
        "\n",
        "* **Tanım:** Sinir ağlarının ağırlıklarını ve biaslarını **güncellemek için kullanılan temel bir algoritmadır**.\n",
        "* **İşleyiş:**\n",
        "    1.  Model bir tahmin yapar.\n",
        "    2.  Tahmin ile gerçek değer arasındaki **hatayı (kaybı)** hesaplar (Kayıp Fonksiyonu ile).\n",
        "    3.  Bu hatayı (gradyanları) ağın çıktı katmanından girdi katmanına doğru **geri yayar**.\n",
        "    4.  Geri yayılım sırasında, her bir ağırlığın ve bias'ın bu hataya ne kadar katkıda bulunduğunu hesaplar.\n",
        "    5.  Bu bilgilerle ağırlıklar ve biaslar, hatayı minimize edecek şekilde güncellenir.\n",
        "\n",
        "### 2.7. Optimizasyon Algoritması (Optimizer)\n",
        "\n",
        "* **Tanım:** Geri yayılımdan gelen hata bilgisini kullanarak, ağın **ağırlıklarını ve biaslarını nasıl güncelleyeceğine karar veren yöntemdir**.\n",
        "* **Amaç:** Kayıp fonksiyonunu minimize etmek ve modelin daha iyi tahminler yapmasını sağlamaktır.\n",
        "* **Örnekler:**\n",
        "    * **Gradyan İnişi (Gradient Descent):** Ağırlıkları, kayıp fonksiyonunun gradyanının (eğimi) tersi yönünde küçük adımlarla günceller.\n",
        "    * **SGD (Stochastic Gradient Descent):** Gradyan inişinin bir çeşididir, her eğitim örneğinden sonra ağırlıkları günceller.\n",
        "    * **Adam:** Popüler ve genellikle hızlı bir optimizasyon algoritmasıdır. Öğrenme oranını dinamik olarak ayarlar.\n",
        "\n",
        "### 2.8. Kayıp Fonksiyonu (Loss Function / Cost Function)\n",
        "\n",
        "* **Tanım:** Modelin **tahminlerinin gerçek değerlerden ne kadar saptığını ölçen** bir fonksiyondur.\n",
        "* **Amaç:** Eğitim sırasında bu fonksiyonun çıktısını (kayıp değerini) **minimize etmeye** çalışılır. Kayıp ne kadar düşükse, modelin performansı o kadar iyidir.\n",
        "* **Örnekler:**\n",
        "    * **Ortalama Kare Hata (Mean Squared Error - MSE):** Regresyon problemleri için yaygın kullanılır.\n",
        "    * **Çapraz Entropi (Cross-Entropy):** Sınıflandırma problemleri için yaygın kullanılır.\n",
        "\n",
        "### 2.9. Epoch\n",
        "\n",
        "* **Tanım:** Eğitim veri kümesinin **tamamının** sinir ağından **bir kez ileri ve geri geçirilmesi** işlemidir (ileri yayılım ve geri yayılımın tamamlanması).\n",
        "* **İşleyiş:** Model, genellikle birden fazla epoch boyunca eğitilir, böylece tüm veri kümesi üzerinde birden fazla kez öğrenme ve ağırlık güncelleme fırsatı bulur.\n",
        "\n",
        "### 2.10. Batch Boyutu (Batch Size)\n",
        "\n",
        "* **Tanım:** Ağırlıkların güncellenmeden önce aynı anda işlenen **eğitim örneği sayısıdır**.\n",
        "* **İşleyiş:** Örneğin, 1000 resminiz varsa ve batch boyutu 32 ise, model 32 resimi işledikten sonra ağırlıklarını günceller ve bu işlemi 1000/32 = 32 kez tekrarlayarak bir epoch'u tamamlar.\n",
        "* **Avantajları:** Daha büyük batch boyutları eğitimi hızlandırabilir (paralel işlemleme sayesinde) ancak genelleme yeteneğini düşürebilir. Daha küçük batch boyutları daha kararlı gradyanlar sağlayabilir.\n",
        "\n",
        "### 2.11. Öğrenme Oranı (Learning Rate)\n",
        "\n",
        "* **Tanım:** Optimizasyon algoritmasının her adımda **ağırlıkları ne kadar değiştireceğini belirleyen** bir hiperparametredir.\n",
        "* **Önem:**\n",
        "    * **Yüksek öğrenme oranı:** Modelin ağırlıkları çok hızlı değiştirmesine neden olabilir, bu da optimum noktayı aşmasına (overshooting) ve istikrarsız bir eğitime yol açabilir.\n",
        "    * **Düşük öğrenme oranı:** Modelin çok yavaş öğrenmesine neden olabilir, bu da eğitimin çok uzun sürmesine veya yerel bir minimumda takılı kalmasına yol açabilir.\n",
        "* **İnce Ayar:** Doğru öğrenme oranını bulmak, modelin başarılı bir şekilde eğitilmesi için kritik öneme sahiptir.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Bazı Temel Derin Öğrenme Mimarileri\n",
        "\n",
        "### 3.1. Evrişimli Sinir Ağları (Convolutional Neural Networks - CNN)\n",
        "\n",
        "* **Özellik:** Özellikle **görüntü işleme görevleri** (görüntü sınıflandırma, nesne tespiti vb.) için tasarlanmıştır.\n",
        "* **Temel Bileşenler:**\n",
        "    * **Evrişim (Convolution) Katmanları:** Görüntüdeki yerel desenleri ve özellikleri (kenarlar, köşeler, dokular) yakalamak için filtreler kullanır.\n",
        "    * **Havuzlama (Pooling) Katmanları:** Görüntü boyutunu küçültür ve en önemli özellikleri özetler (örneğin, MaxPooling).\n",
        "* **Avantajları:** Görüntüdeki hiyerarşik özellikleri otomatik olarak öğrenme yeteneği.\n",
        "\n",
        "### 3.2. Tekrarlayan Sinir Ağları (Recurrent Neural Networks - RNN)\n",
        "\n",
        "* **Özellik:** **Sıralı veriler** (metin, konuşma, zaman serileri gibi) için uygundur.\n",
        "* **Temel Mekanizma:** Önceki adımlardan gelen bilgiyi **\"hafızalarında\"** (gizli durumlarında) tutabilirler. Bu sayede, dizinin önceki elemanlarına dayanarak mevcut elemanları işleyebilirler.\n",
        "* **Sorunlar:** Uzun dizilerde **uzun vadeli bağımlılıkları öğrenmede zorlanabilirler** (vanishing/exploding gradient problemi).\n",
        "\n",
        "### 3.3. Uzun Kısa Süreli Bellek (Long Short-Term Memory - LSTM) ve Gated Recurrent Unit (GRU)\n",
        "\n",
        "* **Özellik:** RNN'lerin **daha gelişmiş ve popüler versiyonlarıdır**.\n",
        "* **Avantajları:**\n",
        "    * **Hafıza Kapıları (Gates):** \"Unutma\", \"girdi\" ve \"çıktı\" kapıları gibi özel mekanizmalar kullanarak bilgiyi daha etkili bir şekilde depolayabilir, hatırlayabilir ve unutabilirler.\n",
        "    * Bu sayede, **uzun vadeli bağımlılıkları öğrenmede** standart RNN'lerden çok daha başarılıdırlar, özellikle uzun metinler veya konuşmalar gibi sıralı verilerde.\n",
        "\n",
        "### 3.4. Transformerlar\n",
        "\n",
        "* **Özellik:** Özellikle **doğal dil işleme (NLP)** alanında devrim yaratmış modern bir mimaridir.\n",
        "* **Temel Mekanizma:**\n",
        "    * **Dikkat Mekanizması (Attention Mechanism):** Bir dizideki farklı pozisyonlar arasındaki ilişkileri doğrudan modelleyerek, bir kelimenin anlamının cümlenin diğer kısımlarıyla nasıl ilişkili olduğunu \"öğrenmelerini\" sağlar. RNN'lerin aksine, uzun mesafeli bağımlılıkları çok daha etkili bir şekilde yakalayabilirler.\n",
        "    * **Paralel İşleme:** RNN'lerin aksine sıralı işlemeye bağımlı değildirler, bu da çok büyük veri kümelerinde çok daha hızlı eğitime olanak tanır.\n",
        "* **Uygulamalar:** Makine çevirisi, metin özetleme, chatbotlar ve büyük dil modelleri (LLM'ler) gibi birçok NLP görevinde yaygın olarak kullanılırlar.\n",
        "\n",
        "---\n",
        "\n",
        "Bu ders notu, derin öğrenmenin ana hatlarını ve temel bileşenlerini anlamanız için bir başlangıç noktasıdır. Her bir kavram ve mimari, kendi içinde derinlemesine incelenebilecek geniş bir alandır. Daha detaylı bilgi için Dr. Murat Altun'un \"Yapay Zekâ ve Makine Öğrenmesi\" gibi kaynaklarına başvurabilirsiniz."
      ],
      "metadata": {
        "id": "9TRIUMDNf60m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jW-c-3e_f7go"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}